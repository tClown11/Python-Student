    Web cawler 的本质就是一种递归方式。使用Web Crawler的时候，
必须非常谨慎地考虑需要消耗多少网络流量，还要尽力思考能不能让采集目标
的服务器负载更低一些

3.1 遍历单个域名

        维基百科六度分隔理论：小世界.py(查看网页代码)



        每个页面充满了侧边栏、页眉、页脚链接，以及链接到分类页面
等，如/wiki/Talk:Kevin_Bacon

     有一些我们不需要的链接，而我们需要“词条链接”。找出“词条链接”和“
其他链接”的差异

            它们都在id是bodyContent的div标签里
            URL链接不包含分号
            URL链接都以/wiki/开头
            采集URL.py(查看结果）


      保证维基百科词条的选择都是一个全新的随机路径


3.2 采集整个网站

        用一个数据库来存储采集的数据

    遍历整个网站的好处：
        生成网站地图
        收集数据

     *  为了避免一个页面被采集两次，链接去重是非常重要的


     *****   警告：如果递归运行的次数非常多，前面的递归程序就可能崩溃
        Python默认是1000次收集网站数据

    注： 一个异常处理语句中包裹多行语句显然是有点危险的。没法识别究竟
是哪行代码出现了异常



3.3 通过互联网采集
    google在1994年成立的时候，就是用一个陈旧的服务器和一个Python网络爬虫

    在你写爬虫随意跟随外链跳转之前，请问自己几个问题：

        1、要收集哪些数据？这些数据可以通过采集几个已经确定的网站完成吗？或者需要
            发现那些我可能不知道的网站吗？
        2、爬虫到某个网站，是立即顺着下一个出站链接跳到一个新网站，还是在网站上待
            一会儿，深入采集网站的内容？
        3、有没有不想采集的一类网站？
        4、引起了某个网站网管的怀疑，如何避免法律责任？


